---
categories:
- ""
- ""
date: "2017-10-31T22:42:51-05:00"
description: Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna
  sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa
  sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem
  consequat tincidunt. Vivamus et sagittis tempus.
draft: false
#image: pic07.jpg
keywords: ""
slug: before-lbs
title: Before LBS
---



<div id="task-1-short-biography-written-using-markdown" class="section level1">
<h1>Task 1: Short biography written using markdown</h1>
</div>
<div id="tomas-arrarte-raffo" class="section level1">
<h1>Tomas Arrarte Raffo</h1>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>My name is <strong>Tomas Arrarte Raffo</strong>, and I was born and raised in Lima, Peru. I lived in Peru for eighteen years and did my schooling there. When the time came, I decided to undertake my undergraduate studies in the United States. I studied Economics and Mathematics at Bowdoin College, a small liberal arts university in Brunswick, Maine. I ended up graduating from Bowdoin a year ahead of my starting class. Bachelors degrees in the United States usually last four years, but I completed mine in three. So, I decided to invest the additional year of schooling that I now had in my hands in a finance education at, what I consider to be, the best business school in Europe, London Business School.</p>
</div>
<div id="hobbies-and-interests" class="section level2">
<h2>Hobbies and interests</h2>
<p>When I am not studying, hunting for jobs, or doing non-profit work, I am, most probably, outdoors. And having lived in coastal cities for twenty years, that usually means that I am near or in the ocean. I enjoy going to the beach with my friends and family. I also enjoy surfing and the feeling of timelessness that it entails. Unlike studying and job-hunting, which I spend <strong>a lot</strong> of time on, surfing forces me to live in the moment. You do not want a wave catching you off guard, trust me.</p>
<p><em>This link will direct you to a picture of a nice wave in Northern Peru that I
have never surfed:
<a href="https://static.wixstatic.com/media/4cd275_7348eb4319e145fba11115d005f8c0c7~mv2_d_3008_2000_s_2.jpg/v1/fill/w_3008,h_2000,al_c,q_90/4cd275_7348eb4319e145fba11115d005f8c0c7~mv2_d_3008_2000_s_2.jpg">cool_wave</a></em></p>
<p>On top of being a beach person, I am a food fan. My favorite part of cuisine, in general, is finding weird food combinations that are surprisingly not bad. Some of my favorite discoveries thus far are:</p>
<ol style="list-style-type: decimal">
<li>Protein powder and cereal (served in a bowl with milk)</li>
<li>Tuna with barbecue sauce (served in a whole-wheat wrap)</li>
<li>Rice and buffalo sauce (served as a side to pan-fried chicken)</li>
</ol>
<p>I would recommend anybody who has the time and courage to try and find odd food combinations that, somehow, work out.</p>
</div>
</div>
<div id="task-2-gapminder-country-comparison" class="section level1">
<h1>Task 2: <code>gapminder</code> country comparison</h1>
<p>You have seen the <code>gapminder</code> dataset that has data on life expectancy, population, and GDP per capita for 142 countries from 1952 to 2007. To get a glimpse of the dataframe, namely to see the variable names, variable types, etc., we use the <code>glimpse</code> function. We also want to have a look at the first 20 rows of data.</p>
<pre class="r"><code>glimpse(gapminder)</code></pre>
<pre><code>## Rows: 1,704
## Columns: 6
## $ country   &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, …
## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …
## $ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …
## $ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…
## $ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…
## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …</code></pre>
<pre class="r"><code>head(gapminder, 20) # look at the first 20 rows of the dataframe</code></pre>
<pre><code>## # A tibble: 20 × 6
##    country     continent  year lifeExp      pop gdpPercap
##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;
##  1 Afghanistan Asia       1952    28.8  8425333      779.
##  2 Afghanistan Asia       1957    30.3  9240934      821.
##  3 Afghanistan Asia       1962    32.0 10267083      853.
##  4 Afghanistan Asia       1967    34.0 11537966      836.
##  5 Afghanistan Asia       1972    36.1 13079460      740.
##  6 Afghanistan Asia       1977    38.4 14880372      786.
##  7 Afghanistan Asia       1982    39.9 12881816      978.
##  8 Afghanistan Asia       1987    40.8 13867957      852.
##  9 Afghanistan Asia       1992    41.7 16317921      649.
## 10 Afghanistan Asia       1997    41.8 22227415      635.
## 11 Afghanistan Asia       2002    42.1 25268405      727.
## 12 Afghanistan Asia       2007    43.8 31889923      975.
## 13 Albania     Europe     1952    55.2  1282697     1601.
## 14 Albania     Europe     1957    59.3  1476505     1942.
## 15 Albania     Europe     1962    64.8  1728137     2313.
## 16 Albania     Europe     1967    66.2  1984060     2760.
## 17 Albania     Europe     1972    67.7  2263554     3313.
## 18 Albania     Europe     1977    68.9  2509048     3533.
## 19 Albania     Europe     1982    70.4  2780097     3631.
## 20 Albania     Europe     1987    72    3075321     3739.</code></pre>
<p>Your task is to produce two graphs of how life expectancy has changed over the years for the <code>country</code> and the <code>continent</code> you come from.</p>
<p>I have created the <code>country_data</code> and <code>continent_data</code> with the code below.</p>
<pre class="r"><code>country_data &lt;- gapminder %&gt;%
            filter(country == &quot;Peru&quot;) # just choosing Peru, as it is where I come from

continent_data &lt;- gapminder %&gt;% 
            filter(continent == &quot;Americas&quot;)</code></pre>
<p>First, create a plot of life expectancy over time for the single country you chose. Map <code>year</code> on the x-axis, and <code>lifeExp</code> on the y-axis. You should also use <code>geom_point()</code> to see the actual data points and <code>geom_smooth(se = FALSE)</code> to plot the underlying trendlines. You need to remove the comments <strong>#</strong> from the lines below for your code to run.</p>
<pre class="r"><code>plot1 &lt;- ggplot(country_data, mapping = aes(x = year, y = lifeExp)) +
  geom_point() +
  geom_smooth(se = FALSE)+
  NULL 

plot1</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/lifeExp_one_country-1.png" width="672" /></p>
<p>Next we need to add a title. Create a new plot, or extend plot1, using the <code>labs()</code> function to add an informative title to the plot.</p>
<pre class="r"><code>plot1 &lt;- plot1 +
    labs(title = &quot;Life Expectancy in Peru&quot;,
    x = &quot;Year &quot;, 
    y = &quot;Life Expectancy&quot;) +
    NULL

plot1</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/lifeExp_one_country_with_label-1.png" width="672" /></p>
<p>Secondly, produce a plot for all countries in the <em>continent</em> you come from. (Hint: map the <code>country</code> variable to the colour aesthetic. You also want to map <code>country</code> to the <code>group</code> aesthetic, so all points for each country are grouped together).</p>
<pre class="r"><code>ggplot(continent_data, mapping = aes(x = year, y = lifeExp , colour = country , group = country))+
   geom_point() + 
   geom_smooth(se = FALSE) +
   NULL</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/lifeExp_one_continent-1.png" width="672" /></p>
<p>Finally, using the original <code>gapminder</code> data, produce a life expectancy over time graph, grouped (or faceted) by continent. We will remove all legends, adding the <code>theme(legend.position="none")</code> in the end of our ggplot.</p>
<pre class="r"><code>ggplot(data = gapminder, mapping = aes(x = year , y = lifeExp , colour = continent))+
   geom_point() + 
   geom_smooth(se = FALSE) +
   facet_wrap(~ continent) +
   theme(legend.position=&quot;none&quot;) + #remove all legends
   NULL</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/lifeExp_facet_by_continent-1.png" width="672" /></p>
<p>Given these trends, what can you say about life expectancy since 1952? Again, don’t just say what’s happening in the graph. Tell some sort of story and speculate about the differences in the patterns.</p>
<blockquote>
<p>Type your answer after this blockquote.</p>
</blockquote>
<p>The general takeaway from these five graphs is that mean life expectancy across all continents has increased over time. This finding is unsurprising to me since I studied economic development extensively as an undergraduate student.</p>
<p>Out of all five continents, Oceania experienced the lowest percentage increase in life expectancy over the 70 years. I hypothesize that this occurred because the continent started with a high level of development. I am personally aware that Australia and New Zealand have always been politically stable countries with sound healthcare systems.</p>
<p>On the other hand, Asia and the Americas experienced the most significant percentage increases in life expectancy over the 70 years. I hypothesize that this occurred because of the considerable healthcare improvements in both regions. Regarding the Americas, I conjecture that healthcare improvements, specifically in Latin and South America, helped bring up the average life expectancy. I hypothesize that the end of military governments or terrorist eras in Peru, Chile, Argentina, Uruguay, among others, led to fewer cases of pre-mature deaths.</p>
<p>Although all continents did experience increases in their average life expectancy, they all differ in terms of standard deviation. Asian countries vary quite considerably regarding their average life expectancy. This variation probably occurs because some countries, like Japan, have always had robust systems, while others, like Vietnam and Myanmar, have gone through periods of severe political instability. Similarly, Africa’s standard deviation for life expectancy is also quite significant. I conjecture that this occurred because some African countries, like Chad and Lesotho, have historically wrestled with poverty issues, while other nations in the continent like South Africa and Morocco have always had more robust institutions in place.</p>
</div>
<div id="task-3-brexit-vote-analysis" class="section level1">
<h1>Task 3: Brexit vote analysis</h1>
<p>We will have a look at the results of the 2016 Brexit vote in the UK. First we read the data using <code>read_csv()</code> and have a quick glimpse at the data</p>
<pre class="r"><code>brexit_results &lt;- read_csv(here::here(&quot;data&quot;,&quot;brexit_results.csv&quot;))


glimpse(brexit_results)</code></pre>
<pre><code>## Rows: 632
## Columns: 11
## $ Seat        &lt;chr&gt; &quot;Aldershot&quot;, &quot;Aldridge-Brownhills&quot;, &quot;Altrincham and Sale W…
## $ con_2015    &lt;dbl&gt; 50.592, 52.050, 52.994, 43.979, 60.788, 22.418, 52.454, 22…
## $ lab_2015    &lt;dbl&gt; 18.333, 22.369, 26.686, 34.781, 11.197, 41.022, 18.441, 49…
## $ ld_2015     &lt;dbl&gt; 8.824, 3.367, 8.383, 2.975, 7.192, 14.828, 5.984, 2.423, 1…
## $ ukip_2015   &lt;dbl&gt; 17.867, 19.624, 8.011, 15.887, 14.438, 21.409, 18.821, 21.…
## $ leave_share &lt;dbl&gt; 57.89777, 67.79635, 38.58780, 65.29912, 49.70111, 70.47289…
## $ born_in_uk  &lt;dbl&gt; 83.10464, 96.12207, 90.48566, 97.30437, 93.33793, 96.96214…
## $ male        &lt;dbl&gt; 49.89896, 48.92951, 48.90621, 49.21657, 48.00189, 49.17185…
## $ unemployed  &lt;dbl&gt; 3.637000, 4.553607, 3.039963, 4.261173, 2.468100, 4.742731…
## $ degree      &lt;dbl&gt; 13.870661, 9.974114, 28.600135, 9.336294, 18.775591, 6.085…
## $ age_18to24  &lt;dbl&gt; 9.406093, 7.325850, 6.437453, 7.747801, 5.734730, 8.209863…</code></pre>
<p>The data comes from <a href="https://www.thecrosstab.com/">Elliott Morris</a>, who cleaned it and made it available through his <a href="https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r">DataCamp class on analysing election and polling data in R</a>.</p>
<p>Our main outcome variable (or y) is <code>leave_share</code>, which is the percent of votes cast in favour of Brexit, or leaving the EU. Each row is a UK <a href="https://en.wikipedia.org/wiki/United_Kingdom_Parliament_constituencies">parliament constituency</a>.</p>
<p>To get a sense of the spread, or distribution, of the data, we can plot a histogram, a density plot, and the empirical cumulative distribution function of the leave % in all constituencies.</p>
<pre class="r"><code># histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5) +
  labs(title = &quot;Distribution of Percent of Constituency that Voted for Brexit&quot;,
    x = &quot;Percent of Constituency that Voted for Brexit&quot;, 
    y = &quot;Number of Constituencies&quot;)</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/brexit_histogram-1.png" width="672" /></p>
<pre class="r"><code># density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density() +
  labs(title = &quot;Density of Percent of Constituency that Voted for Brexit&quot;,
    x = &quot;Percent of Constituency that Voted for Brexit&quot;, 
    y = &quot;Share of Constituencies&quot;)</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/brexit_histogram-2.png" width="672" /></p>
<pre class="r"><code># The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = &quot;step&quot;, pad = FALSE) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = &quot;Empirical Cumulative Distribution of Percent of Constituency that Voted for Brexit&quot;,
    x = &quot;Percent of Constituency that Voted for Brexit&quot;,
    y = &quot;Cumulative Share of Constituencies (%)&quot;)</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/brexit_histogram-3.png" width="672" /></p>
<p>One common explanation for the Brexit outcome was fear of immigration and opposition to the EU’s more open border policy. We can check the relationship (or correlation) between the proportion of native born residents (<code>born_in_uk</code>) in a constituency and its <code>leave_share</code>. To do this, let us get the correlation between the two variables</p>
<pre class="r"><code>brexit_results %&gt;% 
  select(leave_share, born_in_uk) %&gt;% 
  cor()</code></pre>
<pre><code>##             leave_share born_in_uk
## leave_share   1.0000000  0.4934295
## born_in_uk    0.4934295  1.0000000</code></pre>
<p>The correlation is almost 0.5, which shows that the two variables are positively correlated.</p>
<p>We can also create a scatterplot between these two variables using <code>geom_point</code>. We also add the best fit line, using <code>geom_smooth(method = "lm")</code>.</p>
<pre class="r"><code>ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  
  # add a smoothing line, and use method=&quot;lm&quot; to get the best straight-line
  geom_smooth(method = &quot;lm&quot;) + 
  
  # use a white background and frame the plot with a black box
  theme_bw() +
  labs(title = &quot;Correlation between Share of Constituency Born in the UK and Percent 
of Constituency that Voted for Brexit&quot;,
    x = &quot;Share of Constituency Born in the UK&quot;, 
    y = &quot;Percent of Constituency that Voted for Brexit&quot;)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/brexit_immigration_plot-1.png" width="672" /></p>
<pre class="r"><code>  NULL</code></pre>
<pre><code>## NULL</code></pre>
<p>You have the code for the plots, I would like you to revisit all of them and use the <code>labs()</code> function to add an informative title, subtitle, and axes titles to all plots.</p>
<p>What can you say about the relationship shown above? Again, don’t just say what’s happening in the graph. Tell some sort of story and speculate about the differences in the patterns.</p>
<blockquote>
<p>Type your answer after, and outside, this blockquote.</p>
</blockquote>
<p>The graphs above show that, although the population did favor Brexit on average, not all constituencies supported the cause. I believe that the constituencies that did not strongly support Brexit were located in metropolitan areas with high international representation. I hypothesize that individuals in constituencies with little UK-born representation do not judge or fear foreigners as much as individuals from constituencies with high UK-born representation. I contend that exposure to foreigners leads to less fear of foreigners. The positive correlation between the Share of Constituency Born in the UK and the Percent of Constituency that Voted for Brexit suggests that there could be a causal relationship between the variables.</p>
<p>The correlation analysis between the two variables also suggests that a constituency with a high share of UK-born individuals will probably favor Brexit more than a constituency with a low percentage of UK-born individuals. I conjecture that several rural places in the UK were particularly in favor of Brexit due to their high share of UK-born individuals.</p>
</div>
<div id="task-4-animal-rescue-incidents-attended-by-the-london-fire-brigade" class="section level1">
<h1>Task 4: Animal rescue incidents attended by the London Fire Brigade</h1>
<p><a href="https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb">The London Fire Brigade</a> attends a range of non-fire incidents (which we call ‘special services’). These ‘special services’ include assistance to animals that may be trapped or in distress. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.</p>
<p>Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.</p>
<pre class="r"><code>url &lt;- &quot;https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv&quot;

animal_rescue &lt;- read_csv(url,
                          locale = locale(encoding = &quot;CP1252&quot;)) %&gt;% 
  janitor::clean_names()


glimpse(animal_rescue)</code></pre>
<pre><code>## Rows: 7,951
## Columns: 31
## $ incident_number               &lt;chr&gt; &quot;139091&quot;, &quot;275091&quot;, &quot;2075091&quot;, &quot;2872091&quot;…
## $ date_time_of_call             &lt;chr&gt; &quot;01/01/2009 03:01&quot;, &quot;01/01/2009 08:51&quot;, …
## $ cal_year                      &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009…
## $ fin_year                      &lt;chr&gt; &quot;2008/09&quot;, &quot;2008/09&quot;, &quot;2008/09&quot;, &quot;2008/0…
## $ type_of_incident              &lt;chr&gt; &quot;Special Service&quot;, &quot;Special Service&quot;, &quot;S…
## $ pump_count                    &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, …
## $ pump_hours_total              &lt;chr&gt; &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, …
## $ hourly_notional_cost          &lt;dbl&gt; 255, 255, 255, 255, 255, 255, 255, 255, …
## $ incident_notional_cost        &lt;chr&gt; &quot;510&quot;, &quot;255&quot;, &quot;255&quot;, &quot;255&quot;, &quot;255&quot;, &quot;255&quot;…
## $ final_description             &lt;chr&gt; &quot;Redacted&quot;, &quot;Redacted&quot;, &quot;Redacted&quot;, &quot;Red…
## $ animal_group_parent           &lt;chr&gt; &quot;Dog&quot;, &quot;Fox&quot;, &quot;Dog&quot;, &quot;Horse&quot;, &quot;Rabbit&quot;, …
## $ originof_call                 &lt;chr&gt; &quot;Person (land line)&quot;, &quot;Person (land line…
## $ property_type                 &lt;chr&gt; &quot;House - single occupancy&quot;, &quot;Railings&quot;, …
## $ property_category             &lt;chr&gt; &quot;Dwelling&quot;, &quot;Outdoor Structure&quot;, &quot;Outdoo…
## $ special_service_type_category &lt;chr&gt; &quot;Other animal assistance&quot;, &quot;Other animal…
## $ special_service_type          &lt;chr&gt; &quot;Animal assistance involving livestock -…
## $ ward_code                     &lt;chr&gt; &quot;E05011467&quot;, &quot;E05000169&quot;, &quot;E05000558&quot;, &quot;…
## $ ward                          &lt;chr&gt; &quot;Crystal Palace &amp; Upper Norwood&quot;, &quot;Woods…
## $ borough_code                  &lt;chr&gt; &quot;E09000008&quot;, &quot;E09000008&quot;, &quot;E09000029&quot;, &quot;…
## $ borough                       &lt;chr&gt; &quot;Croydon&quot;, &quot;Croydon&quot;, &quot;Sutton&quot;, &quot;Hilling…
## $ stn_ground_name               &lt;chr&gt; &quot;Norbury&quot;, &quot;Woodside&quot;, &quot;Wallington&quot;, &quot;Ru…
## $ uprn                          &lt;chr&gt; &quot;NULL&quot;, &quot;NULL&quot;, &quot;NULL&quot;, &quot;1.00021E+11&quot;, &quot;…
## $ street                        &lt;chr&gt; &quot;Waddington Way&quot;, &quot;Grasmere Road&quot;, &quot;Mill…
## $ usrn                          &lt;chr&gt; &quot;20500146&quot;, &quot;NULL&quot;, &quot;NULL&quot;, &quot;21401484&quot;, …
## $ postcode_district             &lt;chr&gt; &quot;SE19&quot;, &quot;SE25&quot;, &quot;SM5&quot;, &quot;UB9&quot;, &quot;RM3&quot;, &quot;RM…
## $ easting_m                     &lt;chr&gt; &quot;NULL&quot;, &quot;534785&quot;, &quot;528041&quot;, &quot;504689&quot;, &quot;N…
## $ northing_m                    &lt;chr&gt; &quot;NULL&quot;, &quot;167546&quot;, &quot;164923&quot;, &quot;190685&quot;, &quot;N…
## $ easting_rounded               &lt;dbl&gt; 532350, 534750, 528050, 504650, 554650, …
## $ northing_rounded              &lt;dbl&gt; 170050, 167550, 164950, 190650, 192350, …
## $ latitude                      &lt;chr&gt; &quot;NULL&quot;, &quot;51.39095371&quot;, &quot;51.36894086&quot;, &quot;5…
## $ longitude                     &lt;chr&gt; &quot;NULL&quot;, &quot;-0.064166887&quot;, &quot;-0.161985191&quot;, …</code></pre>
<p>One of the more useful things one can do with any data set is quick counts, namely to see how many observations fall within one category. For instance, if we wanted to count the number of incidents by year, we would either use <code>group_by()... summarise()</code> or, simply <a href="https://dplyr.tidyverse.org/reference/count.html"><code>count()</code></a></p>
<pre class="r"><code>animal_rescue %&gt;% 
  dplyr::group_by(cal_year) %&gt;% 
  summarise(count=n())</code></pre>
<pre><code>## # A tibble: 13 × 2
##    cal_year count
##       &lt;dbl&gt; &lt;int&gt;
##  1     2009   568
##  2     2010   611
##  3     2011   620
##  4     2012   603
##  5     2013   585
##  6     2014   583
##  7     2015   540
##  8     2016   604
##  9     2017   539
## 10     2018   610
## 11     2019   604
## 12     2020   758
## 13     2021   726</code></pre>
<pre class="r"><code>animal_rescue %&gt;% 
  count(cal_year, name=&quot;count&quot;)</code></pre>
<pre><code>## # A tibble: 13 × 2
##    cal_year count
##       &lt;dbl&gt; &lt;int&gt;
##  1     2009   568
##  2     2010   611
##  3     2011   620
##  4     2012   603
##  5     2013   585
##  6     2014   583
##  7     2015   540
##  8     2016   604
##  9     2017   539
## 10     2018   610
## 11     2019   604
## 12     2020   758
## 13     2021   726</code></pre>
<p>Let us try to see how many incidents we have by animal group. Again, we can do this either using group_by() and summarise(), or by using count()</p>
<pre class="r"><code>animal_rescue %&gt;% 
  group_by(animal_group_parent) %&gt;% 
  
  #group_by and summarise will produce a new column with the count in each animal group
  summarise(count = n()) %&gt;% 
  
  # mutate adds a new column; here we calculate the percentage
  mutate(percent = round(100*count/sum(count),2)) %&gt;% 
  
  # arrange() sorts the data by percent. Since the default sorting is min to max and we would like to see it sorted
  # in descending order (max to min), we use arrange(desc()) 
  arrange(desc(percent))</code></pre>
<pre><code>## # A tibble: 28 × 3
##    animal_group_parent              count percent
##    &lt;chr&gt;                            &lt;int&gt;   &lt;dbl&gt;
##  1 Cat                               3829   48.2 
##  2 Bird                              1638   20.6 
##  3 Dog                               1242   15.6 
##  4 Fox                                380    4.78
##  5 Unknown - Domestic Animal Or Pet   202    2.54
##  6 Horse                              195    2.45
##  7 Deer                               137    1.72
##  8 Unknown - Wild Animal               95    1.19
##  9 Squirrel                            71    0.89
## 10 Unknown - Heavy Livestock Animal    50    0.63
## # … with 18 more rows</code></pre>
<pre class="r"><code>animal_rescue %&gt;% 
  
  #count does the same thing as group_by and summarise
  # name = &quot;count&quot; will call the column with the counts &quot;count&quot; ( exciting, I know)
  # and &#39;sort=TRUE&#39; will sort them from max to min
  count(animal_group_parent, name=&quot;count&quot;, sort=TRUE) %&gt;% 
  mutate(percent = round(100*count/sum(count),2))</code></pre>
<pre><code>## # A tibble: 28 × 3
##    animal_group_parent              count percent
##    &lt;chr&gt;                            &lt;int&gt;   &lt;dbl&gt;
##  1 Cat                               3829   48.2 
##  2 Bird                              1638   20.6 
##  3 Dog                               1242   15.6 
##  4 Fox                                380    4.78
##  5 Unknown - Domestic Animal Or Pet   202    2.54
##  6 Horse                              195    2.45
##  7 Deer                               137    1.72
##  8 Unknown - Wild Animal               95    1.19
##  9 Squirrel                            71    0.89
## 10 Unknown - Heavy Livestock Animal    50    0.63
## # … with 18 more rows</code></pre>
<p>Do you see anything strange in these tables?</p>
<p>Something peculiar that I spotted in the data is that three animals, cats, birds, and dogs, account for 85% of all animal rescues. I would not have thought that such few animals represented such a significant share of the population of rescues.</p>
<p>Additionally, I find it strange that this table contains two animal groups for cats: “Cat” and “cat”.</p>
<p>Finally, let us have a look at the notional cost for rescuing each of these animals. As the LFB says,</p>
<blockquote>
<p>Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.</p>
</blockquote>
<p>There is two things we will do:</p>
<ol style="list-style-type: decimal">
<li>Calculate the mean and median <code>incident_notional_cost</code> for each <code>animal_group_parent</code></li>
<li>Plot a boxplot to get a feel for the distribution of <code>incident_notional_cost</code> by <code>animal_group_parent</code>.</li>
</ol>
<p>Before we go on, however, we need to fix <code>incident_notional_cost</code> as it is stored as a <code>chr</code>, or character, rather than a number.</p>
<pre class="r"><code># what type is variable incident_notional_cost from dataframe `animal_rescue`
typeof(animal_rescue$incident_notional_cost)</code></pre>
<pre><code>## [1] &quot;character&quot;</code></pre>
<pre class="r"><code># readr::parse_number() will convert any numerical values stored as characters into numbers
animal_rescue &lt;- animal_rescue %&gt;% 

  # we use mutate() to use the parse_number() function and overwrite the same variable
  mutate(incident_notional_cost = parse_number(incident_notional_cost))

# incident_notional_cost from dataframe `animal_rescue` is now &#39;double&#39; or numeric
typeof(animal_rescue$incident_notional_cost)</code></pre>
<pre><code>## [1] &quot;double&quot;</code></pre>
<p>Now tht incident_notional_cost is numeric, let us quickly calculate summary statistics for each animal group.</p>
<pre class="r"><code>animal_rescue %&gt;% 
  
  # group by animal_group_parent
  group_by(animal_group_parent) %&gt;% 
  
  # filter resulting data, so each group has at least 6 observations
  filter(n()&gt;6) %&gt;% 
  
  # summarise() will collapse all values into 3 values: the mean, median, and count  
  # we use na.rm=TRUE to make sure we remove any NAs, or cases where we do not have the incident cos
  summarise(mean_incident_cost = mean (incident_notional_cost, na.rm=TRUE),
            median_incident_cost = median (incident_notional_cost, na.rm=TRUE),
            sd_incident_cost = sd (incident_notional_cost, na.rm=TRUE),
            min_incident_cost = min (incident_notional_cost, na.rm=TRUE),
            max_incident_cost = max (incident_notional_cost, na.rm=TRUE),
            count = n()) %&gt;% 
  
  # sort the resulting data in descending order. You choose whether to sort by count or mean cost.
  arrange(desc(animal_group_parent))</code></pre>
<pre><code>## # A tibble: 16 × 7
##    animal_group_parent      mean_incident_co… median_incident_… sd_incident_cost
##    &lt;chr&gt;                                &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;
##  1 Unknown - Wild Animal                 415.              333             321. 
##  2 Unknown - Heavy Livesto…              374.              260             263. 
##  3 Unknown - Domestic Anim…              326.              295             116. 
##  4 Squirrel                              315.              328              56.0
##  5 Snake                                 356.              339             105. 
##  6 Rabbit                                309.              326              32.2
##  7 Horse                                 740.              596             541. 
##  8 Hamster                               315.              290              95.0
##  9 Fox                                   375.              330.            204. 
## 10 Ferret                                309.              333              39.4
## 11 Dog                                   347.              298             167. 
## 12 Deer                                  415.              333             281. 
## 13 Cow                                   599.              436             451. 
## 14 Cat                                   345.              326             161. 
## 15 cat                                   324.              290              94.1
## 16 Bird                                  344.              328             134. 
## # … with 3 more variables: min_incident_cost &lt;dbl&gt;, max_incident_cost &lt;dbl&gt;,
## #   count &lt;int&gt;</code></pre>
<p>Compare the mean and the median for each animal group? What do you think this is telling us? Anything else that stands out? Any outliers?</p>
<p>For most animals groups, the mean is greater than the median. This fact implies a right-skewed distribution. Moreover, it tells us that, for most animal groups, a few radically expensive incidents cause the mean incident cost to be higher than the median incident cost.</p>
<p>The animal groups that fit into the right-skewed-incident-cost category are:
1. Unknown - Wild Animal
2. Unknown - Heavy Livestock Animal
3. Unknown - Domestic Animal Or Pet
4. Snake
5. Horse
6. Hamster
7. Fox
8. Dog
9. Deer
10. Cow
11. Cat
12. cat
13. Bird</p>
<p>Notice that most animals in the previous list are domestic or livestock. It is easy to understand how a few radically expensive incidents could have arisen from tame animals like dogs, cats, birds, or cows. Those sometimes tend to get into trouble, as evidenced by thousands of videos on YouTube. Notice, however, that the list also contains wild animals, like deer and snakes. These seem to be outliers on this list.</p>
<p>For the other animal groups, the median is greater than the mean. This fact implies a left-skewed distribution. Moreover, it tells us that few radically cheap incidents for a few animal groups make the median incident cost more considerable than the mean incident cost.</p>
<p>The animal groups that fit into the left-skewed-incident-cost category are:
1. Ferrets
3. Rabbit
2. Squirrel</p>
<p>Notice that the animals in this list are usually tiny or rodents. As such, it comes to me as no surprise that a few of their incidents are radically cheap.</p>
<p>Finally, let us plot a few plots that show the distribution of incident_cost for each animal group.</p>
<pre class="r"><code># base_plot
base_plot &lt;- animal_rescue %&gt;% 
  group_by(animal_group_parent) %&gt;% 
  filter(n()&gt;6) %&gt;% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = &quot;free&quot;)+
  theme_bw()

base_plot + geom_histogram()</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/plots_on_incident_cost_by_animal_group-1.png" width="672" /></p>
<pre class="r"><code>base_plot + geom_density()</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/plots_on_incident_cost_by_animal_group-2.png" width="672" /></p>
<pre class="r"><code>base_plot + geom_boxplot()</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/plots_on_incident_cost_by_animal_group-3.png" width="672" /></p>
<pre class="r"><code>base_plot + stat_ecdf(geom = &quot;step&quot;, pad = FALSE) +
  scale_y_continuous(labels = scales::percent)</code></pre>
<p><img src="/blogs/Tomas_Arrarte_Raffo_files/figure-html/plots_on_incident_cost_by_animal_group-4.png" width="672" /></p>
<p>Which of these four graphs do you think best communicates the variability of the <code>incident_notional_cost</code> values? Also, can you please tell some sort of story (which animals are more expensive to rescue than others, the spread of values) and speculate about the differences in the patterns.</p>
<p>I believe that histograms best communicate the variability of the incident notional cost variable. Unlike the other graph types, histograms give you an idea of the spread and the number of observations for each variable. Having easy access to these two data characteristics allows one to ballpark the variability of the variable quite quickly.</p>
<p>According to the charts, it seems to be that horses and cows are the animals associated with higher average costs. I expected this result because horses and cows are heavy animals, and incidents concerning heavy animals require more effort.</p>
<p>One result that surprised me, especially initially, was the high average incident costs for dogs and cats. Given that these are common domestic animals, I was not expecting incident costs associated with them to be so expensive. However, upon re-assessing the information presented, I understand how cats and dogs can suffer from serious injuries or get in difficult situations, leading to large expenses.</p>
</div>
<div id="submit-the-assignment" class="section level1">
<h1>Submit the assignment</h1>
<p>Knit the completed R Markdown file as an HTML document (use the “Knit” button at the top of the script editor window) and upload it to Canvas.</p>
<div id="details" class="section level2">
<h2>Details</h2>
<p>If you want to, please answer the following</p>
<ul>
<li>Who did you collaborate with: Daniil Ternovskyi, Akos Ersek</li>
<li>Approximately how much time did you spend on this problem set: 8 hours</li>
<li>What, if anything, gave you the most trouble: The final question (#4) seemed to be less structured than the others. Paired with the vagueness of the data we were given, this made it the most complicated question by far</li>
</ul>
</div>
</div>
