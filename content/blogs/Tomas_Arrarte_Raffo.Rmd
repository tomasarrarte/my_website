---
categories:
- ""
- ""
date: "2017-10-31T22:42:51-05:00"
description: Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna
  sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa
  sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem
  consequat tincidunt. Vivamus et sagittis tempus.
draft: false
#image: pic07.jpg
keywords: ""
slug: before-lbs
title: Before LBS
---

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(gapminder)  # gapminder dataset
library(here)
library(janitor)
```

# Task 1: Short biography written using markdown

# Tomas Arrarte Raffo

## Overview
My name is **Tomas Arrarte Raffo**, and I was born and raised in Lima, Peru. I lived in Peru for eighteen years and did my schooling there. When the time came, I decided to undertake my undergraduate studies in the United States. I studied Economics and Mathematics at Bowdoin College, a small liberal arts university in Brunswick, Maine. I ended up graduating from Bowdoin a year ahead of my starting class. Bachelors degrees in the United States usually last four years, but I completed mine in three. So, I decided to invest the additional year of schooling that I now had in my hands in a finance education at, what I consider to be, the best business school in Europe, London Business School.

## Hobbies and interests 
When I am not studying, hunting for jobs, or doing non-profit work, I am, most probably, outdoors. And having lived in coastal cities for twenty years, that usually means that I am near or in the ocean. I enjoy going to the beach with my friends and family. I also enjoy surfing and the feeling of timelessness that it entails. Unlike studying and job-hunting, which I spend **a lot** of time on, surfing forces me to live in the moment. You do not want a wave catching you off guard, trust me. 

*This link will direct you to a picture of a nice wave in Northern Peru that I 
have never surfed: 
[cool_wave](https://static.wixstatic.com/media/4cd275_7348eb4319e145fba11115d005f8c0c7~mv2_d_3008_2000_s_2.jpg/v1/fill/w_3008,h_2000,al_c,q_90/4cd275_7348eb4319e145fba11115d005f8c0c7~mv2_d_3008_2000_s_2.jpg)*

On top of being a beach person, I am a food fan. My favorite part of cuisine, in general, is finding weird food combinations that are surprisingly not bad. Some of my favorite discoveries thus far are:

1. Protein powder and cereal (served in a bowl with milk)
2. Tuna with barbecue sauce (served in a whole-wheat wrap)
3. Rice and buffalo sauce (served as a side to pan-fried chicken)

I would recommend anybody who has the time and courage to try and find odd food combinations that, somehow, work out.


# Task 2: `gapminder` country comparison

You have seen the `gapminder` dataset that has data on life expectancy, population, and GDP per capita for 142 countries from 1952 to 2007. To get a glimpse of the dataframe, namely to see the variable names, variable types, etc., we use the `glimpse` function. We also want to have a look at the first 20 rows of data.

```{r}
glimpse(gapminder)

head(gapminder, 20) # look at the first 20 rows of the dataframe

```

Your task is to produce two graphs of how life expectancy has changed over the years for the `country` and the `continent` you come from.

I have created the `country_data` and `continent_data` with the code below.

```{r}
country_data <- gapminder %>%
            filter(country == "Peru") # just choosing Peru, as it is where I come from

continent_data <- gapminder %>% 
            filter(continent == "Americas")
```

First, create a plot of life expectancy over time for the single country you chose. Map `year` on the x-axis, and `lifeExp` on the y-axis. You should also use `geom_point()` to see the actual data points and `geom_smooth(se = FALSE)` to plot the underlying trendlines. You need to remove the comments **\#** from the lines below for your code to run.

```{r, lifeExp_one_country}
plot1 <- ggplot(country_data, mapping = aes(x = year, y = lifeExp)) +
  geom_point() +
  geom_smooth(se = FALSE)+
  NULL 

plot1
```

Next we need to add a title. Create a new plot, or extend plot1, using the `labs()` function to add an informative title to the plot.

```{r, lifeExp_one_country_with_label}
plot1 <- plot1 +
    labs(title = "Life Expectancy in Peru",
    x = "Year ", 
    y = "Life Expectancy") +
    NULL

plot1
```

Secondly, produce a plot for all countries in the *continent* you come from. (Hint: map the `country` variable to the colour aesthetic. You also want to map `country` to the `group` aesthetic, so all points for each country are grouped together).

```{r lifeExp_one_continent}
ggplot(continent_data, mapping = aes(x = year, y = lifeExp , colour = country , group = country))+
   geom_point() + 
   geom_smooth(se = FALSE) +
   NULL

```

Finally, using the original `gapminder` data, produce a life expectancy over time graph, grouped (or faceted) by continent. We will remove all legends, adding the `theme(legend.position="none")` in the end of our ggplot.

```{r lifeExp_facet_by_continent}
ggplot(data = gapminder, mapping = aes(x = year , y = lifeExp , colour = continent))+
   geom_point() + 
   geom_smooth(se = FALSE) +
   facet_wrap(~ continent) +
   theme(legend.position="none") + #remove all legends
   NULL

```

Given these trends, what can you say about life expectancy since 1952? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

> Type your answer after this blockquote.

The general takeaway from these five graphs is that mean life expectancy across all continents has increased over time. This finding is unsurprising to me since I studied economic development extensively as an undergraduate student.

Out of all five continents, Oceania experienced the lowest percentage increase in life expectancy over the 70 years. I hypothesize that this occurred because the continent started with a high level of development. I am personally aware that Australia and New Zealand have always been politically stable countries with sound healthcare systems.

On the other hand, Asia and the Americas experienced the most significant percentage increases in life expectancy over the 70 years. I hypothesize that this occurred because of the considerable healthcare improvements in both regions. Regarding the Americas, I conjecture that healthcare improvements, specifically in Latin and South America, helped bring up the average life expectancy. I hypothesize that the end of military governments or terrorist eras in Peru, Chile, Argentina, Uruguay, among others, led to fewer cases of pre-mature deaths.

Although all continents did experience increases in their average life expectancy, they all differ in terms of standard deviation. Asian countries vary quite considerably regarding their average life expectancy. This variation probably occurs because some countries, like Japan, have always had robust systems, while others, like Vietnam and Myanmar, have gone through periods of severe political instability. Similarly, Africa's standard deviation for life expectancy is also quite significant. I conjecture that this occurred because some African countries, like Chad and Lesotho, have historically wrestled with poverty issues, while other nations in the continent like South Africa and Morocco have always had more robust institutions in place.

# Task 3: Brexit vote analysis

We will have a look at the results of the 2016 Brexit vote in the UK. First we read the data using `read_csv()` and have a quick glimpse at the data

```{r load_brexit_data, warning=FALSE, message=FALSE}
brexit_results <- read_csv(here::here("data","brexit_results.csv"))


glimpse(brexit_results)
```

The data comes from [Elliott Morris](https://www.thecrosstab.com/), who cleaned it and made it available through his [DataCamp class on analysing election and polling data in R](https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r).

Our main outcome variable (or y) is `leave_share`, which is the percent of votes cast in favour of Brexit, or leaving the EU. Each row is a UK [parliament constituency](https://en.wikipedia.org/wiki/United_Kingdom_Parliament_constituencies).

To get a sense of the spread, or distribution, of the data, we can plot a histogram, a density plot, and the empirical cumulative distribution function of the leave % in all constituencies.

```{r brexit_histogram, warning=FALSE, message=FALSE}

# histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5) +
  labs(title = "Distribution of Percent of Constituency that Voted for Brexit",
    x = "Percent of Constituency that Voted for Brexit", 
    y = "Number of Constituencies")


# density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density() +
  labs(title = "Density of Percent of Constituency that Voted for Brexit",
    x = "Percent of Constituency that Voted for Brexit", 
    y = "Share of Constituencies")


# The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Empirical Cumulative Distribution of Percent of Constituency that Voted for Brexit",
    x = "Percent of Constituency that Voted for Brexit",
    y = "Cumulative Share of Constituencies (%)")
  
```

One common explanation for the Brexit outcome was fear of immigration and opposition to the EU's more open border policy. We can check the relationship (or correlation) between the proportion of native born residents (`born_in_uk`) in a constituency and its `leave_share`. To do this, let us get the correlation between the two variables

```{r brexit_immigration_correlation}
brexit_results %>% 
  select(leave_share, born_in_uk) %>% 
  cor()
```

The correlation is almost 0.5, which shows that the two variables are positively correlated.

We can also create a scatterplot between these two variables using `geom_point`. We also add the best fit line, using `geom_smooth(method = "lm")`.

```{r brexit_immigration_plot}
ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  
  # add a smoothing line, and use method="lm" to get the best straight-line
  geom_smooth(method = "lm") + 
  
  # use a white background and frame the plot with a black box
  theme_bw() +
  labs(title = "Correlation between Share of Constituency Born in the UK and Percent 
of Constituency that Voted for Brexit",
    x = "Share of Constituency Born in the UK", 
    y = "Percent of Constituency that Voted for Brexit")
  NULL
```

You have the code for the plots, I would like you to revisit all of them and use the `labs()` function to add an informative title, subtitle, and axes titles to all plots.

What can you say about the relationship shown above? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

> Type your answer after, and outside, this blockquote.

The graphs above show that, although the population did favor Brexit on average, not all constituencies supported the cause. I believe that the constituencies that did not strongly support Brexit were located in metropolitan areas with high international representation. I hypothesize that individuals in constituencies with little UK-born representation do not judge or fear foreigners as much as individuals from constituencies with high UK-born representation. I contend that exposure to foreigners leads to less fear of foreigners. The positive correlation between the Share of Constituency Born in the UK and the Percent of Constituency that Voted for Brexit suggests that there could be a causal relationship between the variables.

The correlation analysis between the two variables also suggests that a constituency with a high share of UK-born individuals will probably favor Brexit more than a constituency with a low percentage of UK-born individuals. I conjecture that several rural places in the UK were particularly in favor of Brexit due to their high share of UK-born individuals.

# Task 4: Animal rescue incidents attended by the London Fire Brigade

[The London Fire Brigade](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb) attends a range of non-fire incidents (which we call 'special services'). These 'special services' include assistance to animals that may be trapped or in distress. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.

Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

```{r load_animal_rescue_data, warning=FALSE, message=FALSE}

url <- "https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv"

animal_rescue <- read_csv(url,
                          locale = locale(encoding = "CP1252")) %>% 
  janitor::clean_names()


glimpse(animal_rescue)
```
One of the more useful things one can do with any data set is quick counts, namely to see how many observations fall within one category. For instance, if we wanted to count the number of incidents by year, we would either use `group_by()... summarise()` or, simply [`count()`](https://dplyr.tidyverse.org/reference/count.html)

```{r, instances_by_calendar_year}

animal_rescue %>% 
  dplyr::group_by(cal_year) %>% 
  summarise(count=n())

animal_rescue %>% 
  count(cal_year, name="count")

```

Let us try to see how many incidents we have by animal group. Again, we can do this either using group_by() and summarise(), or by using count()

```{r, animal_group_percentages}
animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  
  #group_by and summarise will produce a new column with the count in each animal group
  summarise(count = n()) %>% 
  
  # mutate adds a new column; here we calculate the percentage
  mutate(percent = round(100*count/sum(count),2)) %>% 
  
  # arrange() sorts the data by percent. Since the default sorting is min to max and we would like to see it sorted
  # in descending order (max to min), we use arrange(desc()) 
  arrange(desc(percent))


animal_rescue %>% 
  
  #count does the same thing as group_by and summarise
  # name = "count" will call the column with the counts "count" ( exciting, I know)
  # and 'sort=TRUE' will sort them from max to min
  count(animal_group_parent, name="count", sort=TRUE) %>% 
  mutate(percent = round(100*count/sum(count),2))


```

Do you see anything strange in these tables? 

Something peculiar that I spotted in the data is that three animals, cats, birds, and dogs, account for 85% of all animal rescues. I would not have thought that such few animals represented such a significant share of the population of rescues. 

Additionally, I find it strange that this table contains two animal groups for cats: "Cat" and "cat".

Finally, let us have a look at the notional cost for rescuing each of these animals. As the LFB says,

> Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

There is two things we will do:

1. Calculate the mean and median `incident_notional_cost` for each `animal_group_parent`
2. Plot a boxplot to get a feel for the distribution of `incident_notional_cost` by `animal_group_parent`.

Before we go on, however, we need to fix `incident_notional_cost` as it is stored as a `chr`, or character, rather than a number.

```{r, parse_incident_cost,message=FALSE, warning=FALSE}

# what type is variable incident_notional_cost from dataframe `animal_rescue`
typeof(animal_rescue$incident_notional_cost)

# readr::parse_number() will convert any numerical values stored as characters into numbers
animal_rescue <- animal_rescue %>% 

  # we use mutate() to use the parse_number() function and overwrite the same variable
  mutate(incident_notional_cost = parse_number(incident_notional_cost))

# incident_notional_cost from dataframe `animal_rescue` is now 'double' or numeric
typeof(animal_rescue$incident_notional_cost)

```

Now tht incident_notional_cost is numeric, let us quickly calculate summary statistics for each animal group. 


```{r, stats_on_incident_cost,message=FALSE, warning=FALSE}

animal_rescue %>% 
  
  # group by animal_group_parent
  group_by(animal_group_parent) %>% 
  
  # filter resulting data, so each group has at least 6 observations
  filter(n()>6) %>% 
  
  # summarise() will collapse all values into 3 values: the mean, median, and count  
  # we use na.rm=TRUE to make sure we remove any NAs, or cases where we do not have the incident cos
  summarise(mean_incident_cost = mean (incident_notional_cost, na.rm=TRUE),
            median_incident_cost = median (incident_notional_cost, na.rm=TRUE),
            sd_incident_cost = sd (incident_notional_cost, na.rm=TRUE),
            min_incident_cost = min (incident_notional_cost, na.rm=TRUE),
            max_incident_cost = max (incident_notional_cost, na.rm=TRUE),
            count = n()) %>% 
  
  # sort the resulting data in descending order. You choose whether to sort by count or mean cost.
  arrange(desc(animal_group_parent))

```
Compare the mean and the median for each animal group? What do you think this is telling us? Anything else that stands out? Any outliers?

For most animals groups, the mean is greater than the median. This fact implies a right-skewed distribution. Moreover, it tells us that, for most animal groups, a few radically expensive incidents cause the mean incident cost to be higher than the median incident cost.

The animal groups that fit into the right-skewed-incident-cost category are:
1. Unknown - Wild Animal
2. Unknown - Heavy Livestock Animal
3. Unknown - Domestic Animal Or Pet
4. Snake
5. Horse
6. Hamster
7. Fox
8. Dog
9. Deer
10. Cow
11. Cat
12. cat
13. Bird

Notice that most animals in the previous list are domestic or livestock. It is easy to understand how a few radically expensive incidents could have arisen from tame animals like dogs, cats, birds, or cows. Those sometimes tend to get into trouble, as evidenced by thousands of videos on YouTube. Notice, however, that the list also contains wild animals, like deer and snakes. These seem to be outliers on this list.

For the other animal groups, the median is greater than the mean. This fact implies a left-skewed distribution. Moreover, it tells us that few radically cheap incidents for a few animal groups make the median incident cost more considerable than the mean incident cost.

The animal groups that fit into the left-skewed-incident-cost category are:
1. Ferrets
3. Rabbit
2. Squirrel

Notice that the animals in this list are usually tiny or rodents. As such, it comes to me as no surprise that a few of their incidents are radically cheap.


Finally, let us plot a few plots that show the distribution of incident_cost for each animal group.

```{r, plots_on_incident_cost_by_animal_group,message=FALSE, warning=FALSE}

# base_plot
base_plot <- animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = "free")+
  theme_bw()

base_plot + geom_histogram()
base_plot + geom_density()
base_plot + geom_boxplot()
base_plot + stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)

```

Which of these four graphs do you think best communicates the variability of the `incident_notional_cost` values? Also, can you please tell some sort of story (which animals are more expensive to rescue than others, the spread of values) and speculate about the differences in the patterns.

I believe that histograms best communicate the variability of the incident notional cost variable. Unlike the other graph types, histograms give you an idea of the spread and the number of observations for each variable. Having easy access to these two data characteristics allows one to ballpark the variability of the variable quite quickly.

According to the charts, it seems to be that horses and cows are the animals associated with higher average costs. I expected this result because horses and cows are heavy animals, and incidents concerning heavy animals require more effort.

One result that surprised me, especially initially, was the high average incident costs for dogs and cats. Given that these are common domestic animals, I was not expecting incident costs associated with them to be so expensive. However, upon re-assessing the information presented, I understand how cats and dogs can suffer from serious injuries or get in difficult situations, leading to large expenses.

# Submit the assignment

Knit the completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

## Details

If you want to, please answer the following

-   Who did you collaborate with: Daniil Ternovskyi, Akos Ersek
-   Approximately how much time did you spend on this problem set: 8 hours
-   What, if anything, gave you the most trouble: The final question (#4) seemed to be less structured than the others. Paired with the vagueness of the data we were given, this made it the most complicated question by far
